# :tada: _T-ITS ACCEPTED!_ :confetti_ball:

# :page_with_curl: Safe Human-in-the-loop RL (SafeHiL-RL) with Shared Control for End-to-End Autonomous Driving

# :fire: Source Code Released! :fire:

## [[**T-ITS**]](https://ieeexplore.ieee.org/document/10596046) | [[**arXiv**]](https://www.researchgate.net/publication/382212078_Safety-Aware_Human-in-the-Loop_Reinforcement_Learning_With_Shared_Control_for_Autonomous_Driving)

## ğŸ¯ ì‹¤í—˜ ì†Œê°œ ë° ì—°êµ¬ ëª©ì 

### ğŸ“‹ ì—°êµ¬ ë°°ê²½
ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ ë°œì „ê³¼ í•¨ê»˜ ì¸ê³µì§€ëŠ¥(AI)ê³¼ ì¸ê°„ì˜ í˜‘ë ¥ ì‹œìŠ¤í…œì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, LLM(Large Language Model)ê³¼ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì˜ í˜‘ë ¥ ì‹œìŠ¤í…œì—ì„œ ì¸ê°„ì˜ ê°œì…ì´ ì‹œìŠ¤í…œ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•œ ì—°êµ¬ ë¶„ì•¼ì…ë‹ˆë‹¤.

### ğŸ”¬ ì‹¤í—˜ ëª©í‘œ
ë³¸ ì—°êµ¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ëª©í‘œë¥¼ ê°€ì§€ê³  ì§„í–‰ë©ë‹ˆë‹¤:

1. **LLM-ì—ì´ì „íŠ¸ í˜‘ë ¥ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€**
   - LLMê³¼ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ê°„ì˜ í˜‘ë ¥ ì˜ì‚¬ê²°ì • êµ¬ì¡° ë¶„ì„
   - ë‹¤ì–‘í•œ í˜‘ë ¥ ëª¨ë“œ(LLM+Agent, LLM+Agent+Human) ê°„ì˜ ì„±ëŠ¥ ë¹„êµ
   - ì‹œìŠ¤í…œ ì•ˆì •ì„±ê³¼ í•™ìŠµ íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œì˜ ì •ëŸ‰ì  í‰ê°€

2. **ì¸ê°„ ê°œì… íš¨ê³¼ ë¶„ì„**
   - ì¸ê°„ì˜ ê°œì…ì´ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì •ëŸ‰í™”
   - ì•ˆì „ì„± í–¥ìƒê³¼ í•™ìŠµ íš¨ìœ¨ì„± ê°„ì˜ ê· í˜•ì  íƒìƒ‰
   - ì¸ê°„-AI ê³µìœ  ì œì–´(Shared Control) ë©”ì»¤ë‹ˆì¦˜ì˜ íš¨ê³¼ ê²€ì¦

3. **ë‹¤ì–‘í•œ ì˜ì‚¬ê²°ì • êµ¬ì¡° ë¹„êµ**
   - SaHiL (Safety-Aware Human-in-the-Loop)
   - PHIL (Policy-based Human-in-the-Loop) 
   - HIRL (Human-in-the-Loop Reinforcement Learning)
   - SAC (Soft Actor-Critic) ê¸°ë°˜ ì‹œìŠ¤í…œ

### ğŸ› ï¸ ì‹¤í—˜ ë°©ë²•ë¡ 

#### ì‹¤í—˜ í™˜ê²½
- **ì‹œë®¬ë ˆì´í„°**: SMARTS (Scalable Multi-Agent Reinforcement Learning Training School)
- **ì‹œë‚˜ë¦¬ì˜¤**: ê³ ì†ë„ë¡œ ì£¼í–‰ í™˜ê²½ (straight, straight_with_left_turn ë“±)
- **í‰ê°€ ì§€í‘œ**: 
  - ì•ˆì „ì„±: ì¶©ëŒë¥ , ë„ë¡œ ì´íƒˆë¥ 
  - íš¨ìœ¨ì„±: í‰ê·  ë³´ìƒ, ëª©í‘œ ë‹¬ì„±ë¥ 
  - ì¸ê°„ì„±: ì°¨ì„  ì¤‘ì‹¬ ì´íƒˆë„, ê¸‰ê²©í•œ ì¡°í–¥ ë³€í™”

#### ì‹¤í—˜ êµ¬ì„±
1. **LLM+Agent ì‹œìŠ¤í…œ**: LLMê³¼ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì˜ í˜‘ë ¥ ì˜ì‚¬ê²°ì •
2. **LLM+Agent+Human ì‹œìŠ¤í…œ**: ì¸ê°„ ê°œì…ì´ ì¶”ê°€ëœ 3ì í˜‘ë ¥ ì‹œìŠ¤í…œ
3. **ì œì–´ ê¶Œí•œ í• ë‹¹**: ìƒí™©ì— ë”°ë¥¸ ë™ì  ê¶Œí•œ ë¶„ë°° ë©”ì»¤ë‹ˆì¦˜

#### í‰ê°€ ë©”íŠ¸ë¦­
- **ì„±ëŠ¥ ì§€í‘œ**: í‰ê·  ë³´ìƒ, ì„±ê³µë¥ , ì•ˆì „ì„± ì ìˆ˜
- **í•™ìŠµ íš¨ìœ¨ì„±**: ìˆ˜ë ´ ì†ë„, ì •ì±… ì•ˆì •ì„±
- **ì¸ê°„ ê°œì… íš¨ê³¼**: ê°œì… ë¹ˆë„, ê°œì… íš¨ê³¼ì„±, ì‹œìŠ¤í…œ ì•ˆì •ì„±

### ğŸ¯ ê¸°ëŒ€ íš¨ê³¼

#### ì´ë¡ ì  ê¸°ì—¬
- **ì¸ê°„-AI í˜‘ë ¥ ì‹œìŠ¤í…œì˜ ì •ëŸ‰ì  í‰ê°€ í”„ë ˆì„ì›Œí¬** êµ¬ì¶•
- **ì•ˆì „ì„± ì¸ì‹ ê°•í™”í•™ìŠµ** ê¸°ë²•ì˜ íš¨ê³¼ ê²€ì¦
- **ê³µìœ  ì œì–´ ë©”ì»¤ë‹ˆì¦˜**ì˜ ìµœì í™” ë°©ë²•ë¡  ì œì‹œ

#### ì‹¤ìš©ì  ê¸°ì—¬
- **ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œì˜ ì•ˆì „ì„± í–¥ìƒ**ì„ ìœ„í•œ êµ¬ì²´ì  ë°©ì•ˆ ì œì‹œ
- **ì¸ê°„ ê°œì…ì˜ íš¨ê³¼ì  í™œìš©** ë°©ë²•ë¡  ê°œë°œ
- **ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œ**ì˜ ì‹ ë¢°ì„± ì¦ëŒ€

### ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„
ì‹¤í—˜ ê²°ê³¼ëŠ” `results/` ë””ë ‰í† ë¦¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤:
- **ì—í¬í¬ë³„ ì„±ëŠ¥ ë³€í™”**: 400, 600, 700, 775, 820 ì—í¬í¬ì—ì„œì˜ ì„±ëŠ¥ ë¹„êµ
- **ì‹œìŠ¤í…œë³„ ìƒëŒ€ì  ì„±ëŠ¥**: LLM+Agent vs LLM+Agent+Human ì‹œìŠ¤í…œ ë¹„êµ
- **ì•ˆì „ì„± ì§€í‘œ**: ì¶©ëŒë¥ , ë„ë¡œ ì´íƒˆë¥ , ì•ˆì „ ë§ˆì§„ ë“±

---

:dizzy: As a **_pioneering work considering guidance safety_** within the human-in-the-loop RL paradigm, this work introduces a :fire: **_curriculum guidance mechanism_** :fire: inspired by the pedagogical principle of whole-to-part patterns in human education, aiming to standardize the intervention process of human participants.

:red_car: SafeHil-RL is designed to prevent **_policy oscillations or divergence_** caused by **_inappropriate or degraded human guidance_** during interventions using the **_human-AI shared autonomy_** technique, thereby improving learning efficiency, robustness, and driving safety.

:wrench: Realized in SMARTS simulator with Ubuntu 20.04 and Pytorch. 

Email: wenhui001@e.ntu.edu.sg

# Framework

<p align="center">
<img src="https://github.com/OscarHuangWind/Human-in-the-loop-RL/blob/master/presentation/framework.png" height= "450" width="900">
</p>

# Frenet-based Dynamic Potential Field (FDPF)
<p float="left">
  <img src="https://github.com/OscarHuangWind/Human-in-the-loop-RL/blob/master/presentation/FDPF_scenarios.png" height= "140" />
  <img src="https://github.com/OscarHuangWind/Human-in-the-loop-RL/blob/master/presentation/FDPF_bound.png" height= "140" /> 
  <img src="https://github.com/OscarHuangWind/Human-in-the-loop-RL/blob/master/presentation/FDPF_obstacle.png" height= "140" />
  <img src="https://github.com/OscarHuangWind/Human-in-the-loop-RL/blob/master/presentation/FDPF_final.png" height= "140" />
</p>

# Demonstration (accelerated videos)

## Lane-change Performance
https://github.com/OscarHuangWind/Human-in-the-loop-RL/assets/41904672/690b4b44-ac57-4ce1-890b-57ac125cef63
## Uncooperative Road User
https://github.com/OscarHuangWind/Human-in-the-loop-RL/assets/41904672/52b2ec4b-8cd4-4b9d-a3a9-70bbd3b77157
## Cooperative Road User
https://github.com/OscarHuangWind/Human-in-the-loop-RL/assets/41904672/02f95274-80cc-4e6b-8a5b-edfcbbd4d0a6
## Unobserved Road Structure
https://github.com/OscarHuangWind/Human-in-the-loop-RL/assets/41904672/bb493f9c-d2c9-4db5-b034-ad456ef96c8a

# User Guide

## Clone the repository.
cd to your workspace and clone the repo.
```
git clone https://github.com/OscarHuangWind/Safe-Human-in-the-Loop-RL.git
```

## Create a new Conda environment.
cd to your workspace:
```
conda env create -f environment.yml
```

## Activate virtual environment.
```
conda activate safehil-rl
```

## Install Pytorch
Select the correct version based on your cuda version and device (cpu/gpu):
```
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113
```

## Install the SMARTS.
```
# Download SMARTS

git clone https://github.com/huawei-noah/SMARTS.git

cd <path/to/SMARTS>

# Important! Checkout to comp-1 branch
git checkout comp-1

# Install the system requirements.
bash utils/setup/install_deps.sh

# Install smarts.
pip install -e '.[camera_obs,test,train]'

# Install extra dependencies.
pip install -e .[extras]
```

## Build the scenario.
```
cd <path/to/Safe-Human-in-the-loop-RL>
scl scenario build --clean scenario/straight/
```

## Visulazation
```
scl envision start
```
Then go to http://localhost:8081/

## Training
Modify the sys path in **main.py** file, and run:
```
python main.py
```

## Human Guidance
Change the model in **main.py** file to SaHiL/PHIL/HIRL, and run:
```
python main.py
```
Check the code in keyboard.py to get idea of keyboard control.

Alternatively, you can use G29 set to intervene the vehicle control, check the lines from 177 to 191 in main.py file for the details.

The "Egocentric View" is recommended for the human guidance.

## Evaluation
Edit the mode in config.yaml as evaluation and run:
```
python main.py
```




